{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t loss: 633.12\n",
      "\t loss: 615.98\n",
      "\t loss: 602.08\n",
      "\t loss: 589.11\n",
      "\t loss: 576.07\n",
      "\t loss: 562.31\n",
      "\t loss: 547.2\n",
      "\t loss: 529.9\n",
      "\t loss: 509.87\n",
      "\t loss: 486.11\n",
      "\t loss: 459.14\n",
      "\t loss: 429.05\n",
      "\t loss: 396.06\n",
      "\t loss: 360.83\n",
      "\t loss: 324.45\n",
      "\t loss: 291.28\n",
      "\t loss: 300.0\n",
      "\t loss: 655.65\n",
      "\t loss: 740.31\n",
      "\t loss: 523.68\n",
      "\t loss: 471.27\n",
      "\t loss: 420.49\n",
      "\t loss: 373.91\n",
      "\t loss: 333.86\n",
      "\t loss: 300.07\n",
      "\t loss: 270.85\n",
      "\t loss: 244.71\n",
      "\t loss: 220.8\n",
      "\t loss: 198.15\n",
      "\t loss: 177.81\n",
      "\t loss: 169.78\n",
      "\t loss: 221.12\n",
      "\t loss: 435.56\n",
      "\t loss: 313.33\n",
      "\t loss: 215.42\n",
      "\t loss: 166.69\n",
      "\t loss: 133.71\n",
      "\t loss: 110.29\n",
      "\t loss: 93.26\n",
      "\t loss: 84.57\n",
      "\t loss: 103.81\n",
      "\t loss: 190.85\n",
      "\t loss: 308.04\n",
      "\t loss: 160.44\n",
      "\t loss: 94.92\n",
      "\t loss: 67.77\n",
      "\t loss: 54.42\n",
      "\t loss: 49.31\n",
      "\t loss: 58.21\n",
      "\t loss: 107.71\n",
      "\t loss: 173.44\n",
      "\t loss: 184.22\n",
      "\t loss: 69.39\n",
      "\t loss: 40.02\n",
      "\t loss: 30.54\n",
      "\t loss: 26.21\n",
      "\t loss: 24.77\n",
      "\t loss: 26.07\n",
      "\t loss: 33.03\n",
      "\t loss: 44.05\n",
      "\t loss: 67.13\n",
      "\t loss: 64.63\n",
      "\t loss: 62.32\n",
      "\t loss: 35.72\n",
      "\t loss: 27.44\n",
      "\t loss: 23.06\n",
      "\t loss: 24.57\n",
      "\t loss: 27.01\n",
      "\t loss: 32.2\n",
      "\t loss: 36.23\n",
      "\t loss: 37.36\n",
      "\t loss: 35.79\n",
      "\t loss: 28.63\n",
      "\t loss: 26.36\n",
      "\t loss: 22.99\n",
      "\t loss: 27.58\n",
      "\t loss: 34.53\n",
      "\t loss: 52.89\n",
      "\t loss: 49.37\n",
      "\t loss: 51.42\n",
      "\t loss: 24.6\n",
      "\t loss: 17.88\n",
      "\t loss: 11.15\n",
      "\t loss: 9.71\n",
      "\t loss: 8.07\n",
      "\t loss: 7.85\n",
      "\t loss: 7.34\n",
      "\t loss: 7.75\n",
      "\t loss: 7.81\n",
      "\t loss: 8.7\n",
      "\t loss: 9.08\n",
      "\t loss: 10.27\n",
      "\t loss: 10.73\n",
      "\t loss: 12.35\n",
      "\t loss: 12.65\n",
      "\t loss: 13.97\n",
      "\t loss: 12.96\n",
      "\t loss: 13.72\n",
      "\t loss: 12.13\n",
      "\t loss: 12.59\n",
      "\t loss: 11.12\n",
      "\t loss: 11.53\n",
      "\t loss: 10.63\n",
      "\t loss: 11.13\n",
      "\t loss: 10.98\n",
      "\t loss: 12.09\n",
      "\t loss: 13.35\n",
      "\t loss: 14.53\n",
      "\t loss: 16.19\n",
      "\t loss: 16.03\n",
      "\t loss: 17.05\n",
      "\t loss: 14.96\n",
      "\t loss: 14.88\n",
      "\t loss: 12.17\n",
      "\t loss: 11.7\n",
      "\t loss: 9.58\n",
      "\t loss: 9.54\n",
      "\t loss: 8.02\n",
      "\t loss: 8.27\n",
      "\t loss: 7.15\n",
      "\t loss: 7.79\n",
      "\t loss: 6.99\n",
      "\t loss: 7.83\n",
      "\t loss: 7.1\n",
      "\t loss: 8.1\n",
      "\t loss: 7.11\n",
      "\t loss: 7.91\n",
      "\t loss: 6.91\n",
      "\t loss: 7.5\n",
      "\t loss: 6.4\n",
      "\t loss: 6.72\n",
      "\t loss: 5.69\n",
      "\t loss: 5.92\n",
      "\t loss: 5.05\n",
      "\t loss: 5.16\n",
      "\t loss: 4.47\n",
      "\t loss: 4.56\n",
      "\t loss: 3.98\n",
      "\t loss: 4.07\n",
      "\t loss: 3.66\n",
      "\t loss: 3.74\n",
      "\t loss: 3.4\n",
      "\t loss: 3.54\n",
      "\t loss: 3.26\n",
      "\t loss: 3.37\n",
      "\t loss: 3.21\n",
      "\t loss: 3.4\n",
      "\t loss: 3.26\n",
      "\t loss: 3.45\n",
      "\t loss: 3.35\n",
      "\t loss: 3.59\n",
      "\t loss: 3.49\n",
      "\t loss: 3.78\n",
      "\t loss: 3.71\n",
      "\t loss: 3.98\n",
      "\t loss: 3.8\n",
      "\t loss: 4.01\n",
      "\t loss: 3.78\n",
      "\t loss: 3.96\n",
      "\t loss: 3.68\n",
      "\t loss: 3.78\n",
      "\t loss: 3.49\n",
      "\t loss: 3.54\n",
      "\t loss: 3.26\n",
      "\t loss: 3.33\n",
      "\t loss: 3.09\n",
      "\t loss: 3.12\n",
      "\t loss: 2.9\n",
      "\t loss: 2.94\n",
      "\t loss: 2.76\n",
      "\t loss: 2.81\n",
      "\t loss: 2.71\n",
      "\t loss: 2.78\n",
      "\t loss: 2.68\n",
      "\t loss: 2.78\n",
      "\t loss: 2.75\n",
      "\t loss: 2.87\n",
      "\t loss: 2.84\n",
      "\t loss: 2.99\n",
      "\t loss: 3.0\n",
      "\t loss: 3.15\n",
      "\t loss: 3.11\n",
      "\t loss: 3.28\n",
      "\t loss: 3.3\n",
      "\t loss: 3.44\n",
      "\t loss: 3.38\n",
      "\t loss: 3.5\n",
      "\t loss: 3.43\n",
      "\t loss: 3.51\n",
      "\t loss: 3.42\n",
      "\t loss: 3.48\n",
      "\t loss: 3.33\n",
      "\t loss: 3.36\n",
      "\t loss: 3.2\n",
      "\t loss: 3.19\n",
      "\t loss: 3.0\n",
      "\t loss: 2.98\n",
      "\t loss: 2.81\n",
      "\t loss: 2.8\n",
      "\t loss: 2.61\n",
      "\t loss: 2.56\n",
      "\t loss: 2.38\n",
      "\t loss: 2.32\n",
      "\t loss: 2.16\n",
      "\t loss: 2.11\n",
      "\t loss: 1.94\n",
      "\t loss: 1.9\n",
      "\t loss: 1.75\n",
      "\t loss: 1.7\n",
      "\t loss: 1.56\n",
      "\t loss: 1.52\n",
      "\t loss: 1.41\n",
      "\t loss: 1.37\n",
      "\t loss: 1.26\n",
      "\t loss: 1.23\n",
      "\t loss: 1.14\n",
      "\t loss: 1.1\n",
      "\t loss: 1.02\n",
      "\t loss: 0.99\n",
      "\t loss: 0.92\n",
      "\t loss: 0.89\n",
      "\t loss: 0.83\n",
      "\t loss: 0.81\n",
      "\t loss: 0.76\n",
      "\t loss: 0.73\n",
      "\t loss: 0.68\n",
      "\t loss: 0.67\n",
      "\t loss: 0.62\n",
      "\t loss: 0.61\n",
      "\t loss: 0.57\n",
      "\t loss: 0.56\n",
      "\t loss: 0.52\n",
      "\t loss: 0.51\n",
      "\t loss: 0.48\n",
      "\t loss: 0.47\n",
      "\t loss: 0.45\n",
      "\t loss: 0.44\n",
      "\t loss: 0.42\n",
      "\t loss: 0.41\n",
      "\t loss: 0.39\n",
      "\t loss: 0.39\n",
      "\t loss: 0.37\n",
      "\t loss: 0.37\n",
      "\t loss: 0.36\n",
      "\t loss: 0.36\n",
      "\t loss: 0.35\n",
      "\t loss: 0.36\n",
      "\t loss: 0.35\n",
      "\t loss: 0.35\n",
      "\t loss: 0.35\n",
      "\t loss: 0.36\n",
      "\t loss: 0.36\n",
      "\t loss: 0.37\n",
      "\t loss: 0.37\n",
      "\t loss: 0.39\n",
      "\t loss: 0.4\n",
      "\t loss: 0.42\n",
      "\t loss: 0.43\n",
      "\t loss: 0.46\n",
      "\t loss: 0.47\n",
      "\t loss: 0.51\n",
      "\t loss: 0.52\n",
      "\t loss: 0.56\n",
      "\t loss: 0.57\n",
      "\t loss: 0.62\n",
      "\t loss: 0.63\n",
      "\t loss: 0.69\n",
      "\t loss: 0.7\n",
      "\t loss: 0.77\n",
      "\t loss: 0.79\n",
      "\t loss: 0.86\n",
      "\t loss: 0.87\n",
      "\t loss: 0.96\n",
      "\t loss: 0.96\n",
      "\t loss: 1.06\n",
      "\t loss: 1.06\n",
      "\t loss: 1.16\n",
      "\t loss: 1.15\n",
      "\t loss: 1.25\n",
      "\t loss: 1.24\n",
      "\t loss: 1.34\n",
      "\t loss: 1.32\n",
      "\t loss: 1.41\n",
      "\t loss: 1.37\n",
      "\t loss: 1.51\n",
      "\t loss: 1.45\n",
      "\t loss: 1.56\n",
      "\t loss: 1.51\n",
      "\t loss: 1.61\n",
      "\t loss: 1.54\n",
      "\t loss: 1.64\n",
      "\t loss: 1.59\n",
      "\t loss: 1.69\n",
      "\t loss: 1.58\n",
      "\t loss: 1.68\n",
      "\t loss: 1.58\n",
      "\t loss: 1.67\n",
      "\t loss: 1.58\n",
      "\t loss: 1.65\n",
      "\t loss: 1.54\n",
      "\t loss: 1.61\n",
      "\t loss: 1.5\n",
      "\t loss: 1.56\n",
      "\t loss: 1.47\n",
      "\t loss: 1.52\n",
      "\t loss: 1.42\n",
      "\t loss: 1.46\n",
      "\t loss: 1.36\n",
      "\t loss: 1.39\n",
      "\t loss: 1.29\n",
      "\t loss: 1.32\n",
      "\t loss: 1.22\n",
      "\t loss: 1.24\n",
      "\t loss: 1.16\n",
      "\t loss: 1.18\n",
      "\t loss: 1.09\n",
      "\t loss: 1.1\n",
      "\t loss: 1.03\n",
      "\t loss: 1.04\n",
      "\t loss: 0.97\n",
      "\t loss: 0.97\n",
      "\t loss: 0.9\n",
      "\t loss: 0.91\n",
      "\t loss: 0.84\n",
      "\t loss: 0.84\n",
      "\t loss: 0.79\n",
      "\t loss: 0.78\n",
      "\t loss: 0.73\n",
      "\t loss: 0.73\n",
      "\t loss: 0.68\n",
      "\t loss: 0.68\n",
      "\t loss: 0.63\n",
      "\t loss: 0.63\n",
      "\t loss: 0.59\n",
      "\t loss: 0.58\n",
      "\t loss: 0.55\n",
      "\t loss: 0.54\n",
      "\t loss: 0.51\n",
      "\t loss: 0.51\n",
      "\t loss: 0.48\n",
      "\t loss: 0.47\n",
      "\t loss: 0.44\n",
      "\t loss: 0.44\n",
      "\t loss: 0.42\n",
      "\t loss: 0.41\n",
      "\t loss: 0.39\n",
      "\t loss: 0.39\n",
      "\t loss: 0.37\n",
      "\t loss: 0.36\n",
      "\t loss: 0.34\n",
      "\t loss: 0.34\n",
      "\t loss: 0.32\n",
      "\t loss: 0.32\n",
      "\t loss: 0.3\n",
      "\t loss: 0.3\n",
      "\t loss: 0.29\n",
      "\t loss: 0.28\n",
      "\t loss: 0.27\n",
      "\t loss: 0.27\n",
      "\t loss: 0.26\n",
      "\t loss: 0.25\n",
      "\t loss: 0.24\n",
      "\t loss: 0.24\n",
      "\t loss: 0.23\n",
      "\t loss: 0.23\n",
      "\t loss: 0.22\n",
      "\t loss: 0.21\n",
      "\t loss: 0.21\n",
      "\t loss: 0.2\n",
      "\t loss: 0.2\n",
      "\t loss: 0.19\n",
      "\t loss: 0.19\n",
      "\t loss: 0.18\n",
      "\t loss: 0.18\n",
      "\t loss: 0.18\n",
      "\t loss: 0.17\n",
      "\t loss: 0.17\n",
      "\t loss: 0.16\n",
      "\t loss: 0.16\n",
      "\t loss: 0.16\n",
      "\t loss: 0.15\n",
      "\t loss: 0.15\n",
      "\t loss: 0.15\n",
      "\t loss: 0.14\n",
      "\t loss: 0.14\n",
      "\t loss: 0.14\n",
      "\t loss: 0.13\n",
      "\t loss: 0.13\n",
      "\t loss: 0.13\n",
      "\t loss: 0.13\n",
      "\t loss: 0.12\n",
      "\t loss: 0.12\n",
      "\t loss: 0.12\n",
      "\t loss: 0.12\n",
      "\t loss: 0.11\n",
      "\t loss: 0.11\n",
      "\t loss: 0.11\n",
      "\t loss: 0.11\n",
      "\t loss: 0.11\n",
      "\t loss: 0.1\n",
      "\t loss: 0.1\n",
      "\t loss: 0.1\n",
      "\t loss: 0.1\n",
      "\t loss: 0.1\n",
      "\t loss: 0.1\n",
      "\t loss: 0.09\n",
      "\t loss: 0.09\n",
      "\t loss: 0.09\n",
      "\t loss: 0.09\n",
      "\t loss: 0.09\n",
      "\t loss: 0.09\n",
      "\t loss: 0.09\n",
      "\t loss: 0.08\n",
      "\t loss: 0.08\n",
      "\t loss: 0.08\n",
      "\t loss: 0.08\n",
      "\t loss: 0.08\n",
      "\t loss: 0.08\n",
      "\t loss: 0.08\n",
      "\t loss: 0.08\n",
      "\t loss: 0.07\n",
      "\t loss: 0.07\n",
      "\t loss: 0.07\n",
      "\t loss: 0.07\n",
      "\t loss: 0.07\n",
      "\t loss: 0.07\n",
      "\t loss: 0.07\n",
      "\t loss: 0.07\n",
      "\t loss: 0.07\n",
      "\t loss: 0.07\n",
      "\t loss: 0.06\n",
      "\t loss: 0.06\n",
      "\t loss: 0.06\n",
      "\t loss: 0.06\n",
      "\t loss: 0.06\n",
      "\t loss: 0.06\n",
      "\t loss: 0.06\n",
      "\t loss: 0.06\n",
      "\t loss: 0.06\n",
      "\t loss: 0.06\n",
      "\t loss: 0.06\n",
      "\t loss: 0.06\n",
      "\t loss: 0.06\n",
      "\t loss: 0.05\n",
      "\t loss: 0.05\n",
      "\t loss: 0.05\n",
      "\t loss: 0.05\n",
      "\t loss: 0.05\n",
      "\t loss: 0.05\n",
      "\t loss: 0.05\n",
      "\t loss: 0.05\n",
      "\t loss: 0.05\n",
      "\t loss: 0.05\n",
      "\t loss: 0.05\n",
      "\t loss: 0.05\n",
      "\t loss: 0.05\n",
      "\t loss: 0.05\n",
      "\t loss: 0.05\n",
      "\t loss: 0.05\n",
      "\t loss: 0.04\n",
      "\t loss: 0.04\n",
      "\t loss: 0.04\n",
      "\t loss: 0.04\n",
      "\t loss: 0.04\n",
      "\t loss: 0.04\n",
      "\t loss: 0.04\n",
      "\t loss: 0.04\n",
      "\t loss: 0.04\n",
      "\t loss: 0.04\n",
      "\t loss: 0.04\n",
      "\t loss: 0.04\n",
      "\t loss: 0.04\n",
      "\t loss: 0.04\n",
      "\t loss: 0.04\n",
      "\t loss: 0.04\n",
      "\t loss: 0.04\n",
      "\t loss: 0.04\n",
      "\t loss: 0.04\n",
      "\t loss: 0.04\n",
      "\t loss: 0.04\n",
      "\t loss: 0.04\n",
      "\t loss: 0.04\n",
      "\t loss: 0.03\n",
      "\t loss: 0.03\n",
      "\t loss: 0.03\n",
      "\t loss: 0.03\n",
      "\t loss: 0.03\n",
      "\t loss: 0.03\n",
      "\t loss: 0.03\n",
      "\t loss: 0.03\n",
      "\t loss: 0.03\n",
      "\t loss: 0.03\n",
      "\t loss: 0.03\n",
      "\t loss: 0.03\n",
      "\t loss: 0.03\n",
      "\t loss: 0.03\n",
      "\t loss: 0.03\n",
      "\t loss: 0.03\n",
      "\t loss: 0.03\n",
      "\t loss: 0.03\n"
     ]
    }
   ],
   "source": [
    "# Multilayer Perceptron using automatic gradient descent algorithm based on pytorch\n",
    "import torch\n",
    "# add torch.autograd toolbox\n",
    "\n",
    "device = torch.device('cpu')\n",
    "# device = torch.device('cuda') # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H1, D_out = 64, 1000, 100, 10\n",
    "# define 2 more hidden layer\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x_data = torch.randn(N, D_in, device=device)\n",
    "y_data = torch.randn(N, D_out, device=device)\n",
    "\n",
    "lr = 0.001  # learning rate\n",
    "\n",
    "H2 = 84\n",
    "H3 = 42\n",
    "\n",
    "#Step 1: Design our NN model class in pytorch way\n",
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear = torch.nn.Sequential( #sequential container\n",
    "                    torch.nn.Linear(D_in, H1),\n",
    "                    # stack ReLU layer\n",
    "                    torch.nn.ReLU(),\n",
    "                    # stack Linear layer\n",
    "                    torch.nn.Linear(H1, H2),\n",
    "                    # stack ReLU layer\n",
    "                    torch.nn.ReLU(),\n",
    "                    # stack Linear layer\n",
    "                    torch.nn.Linear(H2, H3),\n",
    "                    # stack ReLU layer\n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.Linear(H3, D_out)\n",
    "                ).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# our model\n",
    "model = Model()\n",
    "\n",
    "# Step 2: Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "criterion = torch.nn.MSELoss(reduction='sum')  # sum operation = False\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# Step 3: Training cycle: forward, loss, backward, step\n",
    "for epoch in range(500):\n",
    "    # 1. forward\n",
    "    y_pred = model(x_data)\n",
    "    # 2. loss\n",
    "    loss = criterion(y_pred,y_data)\n",
    "    # 2-1 zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    # 3. backward\n",
    "    loss.backward()\n",
    "    # 4. update\n",
    "    optimizer.step()\n",
    "\n",
    "    print(\"\\t loss: {}\".format(round(loss.item(),2)))  # round off to 2 places of decimals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
